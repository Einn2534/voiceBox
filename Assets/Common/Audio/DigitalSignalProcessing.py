# 
# Digital Signal Processing
#   generated by einn on 2025/10/24
# 
"""Signal processing helpers for the GIAN vocal synthesis project (refactored)."""
from __future__ import annotations

import os
import wave
import unicodedata
from dataclasses import dataclass, field, replace
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

import numpy as np
from math import pi, sin, cos, exp

# =======================
# Constants / Globals
# =======================

DTYPE = np.float32
PEAK_DEFAULT = 0.9
EPS = 1e-12

VOCAL_TRACT_LENGTH_CM = 17.5
DEFAULT_TRACT_SECTIONS = 20
SPEED_OF_SOUND_CM_S = 35000.0

_NEUTRAL_TRACT_AREA_CM2 = np.array([
    2.2, 2.3, 2.5, 2.8, 3.1,
    3.4, 3.5, 3.3, 3.0, 2.7,
    2.4, 2.2, 1.9, 1.7, 1.5,
    1.3, 1.1, 0.95, 0.85, 0.8,
], dtype=np.float64)


@dataclass(frozen=True)
class NasalCoupling:
    """Configuration parameters describing the nasal/sinus branching network."""

    port_open: float = 1.0
    vowel_leak: float = 0.0
    nostril_area_cm2: float = 0.7
    nasal_cavity_length_cm: float = 14.5
    nasal_cavity_area_cm2: float = 2.4
    sinus_cavity_length_cm: float = 5.5
    sinus_cavity_area_cm2: float = 4.2
    sinus_coupling_area_cm2: float = 0.5
    loss_db_per_meter: float = 1.6


@dataclass(frozen=True)
class SpeakerProfile:
    """Minimal speaker description used by the procedural voice model."""

    nasal_coupling: NasalCoupling = field(default_factory=NasalCoupling)


# =======================
# Vowel / Token Tables
# =======================

# ---- 母音プリセット（成人中性声の目安） ----
VOWEL_TABLE: Dict[str, Dict[str, Sequence[float]]] = {
    'a': {'F': [800, 1150, 2900], 'BW': [90, 110, 150]},
    'i': {'F': [350, 2000, 3000], 'BW': [60, 100, 150]},
    'u': {'F': [325, 700, 2530],  'BW': [60, 90, 140]},
    'e': {'F': [400, 1700, 2600], 'BW': [70, 100, 150]},
    'o': {'F': [450, 800, 2830],  'BW': [80, 110, 150]},
}

# ローマ字トークン → (子音, 母音)
CV_TOKEN_MAP: Dict[str, Tuple[str, str]] = {
    'ka': ('k', 'a'),  'ki': ('k', 'i'),  'ku': ('k', 'u'),  'ke': ('k', 'e'),  'ko': ('k', 'o'),
    'sa': ('s', 'a'),  'shi': ('sh', 'i'), 'su': ('s', 'u'), 'se': ('s', 'e'), 'so': ('s', 'o'),
    'ta': ('t', 'a'),  'chi': ('ch', 'i'), 'tsu': ('ts', 'u'), 'te': ('t', 'e'), 'to': ('t', 'o'),
    'na': ('n', 'a'),  'ni': ('n', 'i'),  'nu': ('n', 'u'),  'ne': ('n', 'e'),  'no': ('n', 'o'),
    'ha': ('h', 'a'),  'hi': ('h', 'i'),  'fu': ('f', 'u'),  'he': ('h', 'e'),  'ho': ('h', 'o'),
    'ma': ('m', 'a'),  'mi': ('m', 'i'),  'mu': ('m', 'u'),  'me': ('m', 'e'),  'mo': ('m', 'o'),
    'ya': ('y', 'a'),  'yu': ('y', 'u'),  'yo': ('y', 'o'),
    'ra': ('r', 'a'),  'ri': ('r', 'i'),  'ru': ('r', 'u'),  're': ('r', 'e'),  'ro': ('r', 'o'),
    'wa': ('w', 'a'),  'wo': ('w', 'o'),
}

NASAL_TOKEN_MAP: Dict[str, str] = {'n': 'n', 'nn': 'n', 'm': 'm'}
PAUSE_TOKEN = 'pau'

_VALID_TOKENS = set(CV_TOKEN_MAP) | set(VOWEL_TABLE) | set(NASAL_TOKEN_MAP)
_MAX_ROMAJI_TOKEN_LENGTH = max(len(token) for token in _VALID_TOKENS)

# ひらがな変換・合字
_KANA_BASE_MAP: Dict[str, List[str]] = {
    'あ': ['a'], 'い': ['i'], 'う': ['u'], 'え': ['e'], 'お': ['o'],
    'か': ['ka'], 'き': ['ki'], 'く': ['ku'], 'け': ['ke'], 'こ': ['ko'],
    'さ': ['sa'], 'し': ['shi'], 'す': ['su'], 'せ': ['se'], 'そ': ['so'],
    'た': ['ta'], 'ち': ['chi'], 'つ': ['tsu'], 'て': ['te'], 'と': ['to'],
    'な': ['na'], 'に': ['ni'], 'ぬ': ['nu'], 'ね': ['ne'], 'の': ['no'],
    'は': ['ha'], 'ひ': ['hi'], 'ふ': ['fu'], 'へ': ['he'], 'ほ': ['ho'],
    'ま': ['ma'], 'み': ['mi'], 'む': ['mu'], 'め': ['me'], 'も': ['mo'],
    'や': ['ya'], 'ゆ': ['yu'], 'よ': ['yo'],
    'ら': ['ra'], 'り': ['ri'], 'る': ['ru'], 'れ': ['re'], 'ろ': ['ro'],
    'わ': ['wa'], 'ゐ': ['i'], 'ゑ': ['e'], 'を': ['wo'],
    'ん': ['n'],
    'ぁ': ['a'], 'ぃ': ['i'], 'ぅ': ['u'], 'ぇ': ['e'], 'ぉ': ['o'],
    'ゎ': ['wa'], 'ゕ': ['ka'], 'ゖ': ['ke'],
    'ゔ': ['u'],
}
_VOICED_KANA_MAP: Dict[str, str] = {
    'が': 'か', 'ぎ': 'き', 'ぐ': 'く', 'げ': 'け', 'ご': 'こ',
    'ざ': 'さ', 'じ': 'し', 'ず': 'す', 'ぜ': 'せ', 'ぞ': 'そ',
    'だ': 'た', 'ぢ': 'ち', 'づ': 'つ', 'で': 'て', 'ど': 'と',
    'ば': 'は', 'び': 'ひ', 'ぶ': 'ふ', 'べ': 'へ', 'ぼ': 'ほ',
}
_HAND_DAKUTEN_MAP: Dict[str, str] = {'ぱ': 'は', 'ぴ': 'ひ', 'ぷ': 'ふ', 'ぺ': 'へ', 'ぽ': 'ほ'}

_KANA_DIGRAPH_MAP: Dict[str, List[str]] = {
    'きゃ': ['ki', 'ya'], 'きゅ': ['ki', 'yu'], 'きょ': ['ki', 'yo'],
    'ぎゃ': ['ki', 'ya'], 'ぎゅ': ['ki', 'yu'], 'ぎょ': ['ki', 'yo'],
    'しゃ': ['shi', 'ya'], 'しゅ': ['shi', 'yu'], 'しょ': ['shi', 'yo'],
    'じゃ': ['shi', 'ya'], 'じゅ': ['shi', 'yu'], 'じょ': ['shi', 'yo'],
    'ちゃ': ['chi', 'ya'], 'ちゅ': ['chi', 'yu'], 'ちょ': ['chi', 'yo'],
    'にゃ': ['ni', 'ya'], 'にゅ': ['ni', 'yu'], 'にょ': ['ni', 'yo'],
    'ひゃ': ['hi', 'ya'], 'ひゅ': ['hi', 'yu'], 'ひょ': ['hi', 'yo'],
    'びゃ': ['hi', 'ya'], 'びゅ': ['hi', 'yu'], 'びょ': ['hi', 'yo'],
    'ぴゃ': ['hi', 'ya'], 'ぴゅ': ['hi', 'yu'], 'ぴょ': ['hi', 'yo'],
    'みゃ': ['mi', 'ya'], 'みゅ': ['mi', 'yu'], 'みょ': ['mi', 'yo'],
    'りゃ': ['ri', 'ya'], 'りゅ': ['ri', 'yu'], 'りょ': ['ri', 'yo'],
}

_PUNCTUATION_CHARS = set('、。，．,.!?！？；：:;…‥・「」『』（）()[]{}<>')
_PUNCTUATION_CHARS.update({'"', "'", '“', '”', '‘', '’', '—'})

# =======================
# Text helpers
# =======================


def _clamp(value: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, value))


def _clamp01(value: float) -> float:
    return _clamp(float(value), 0.0, 1.0)


def _sanitize_nasal_coupling(coupling: Optional[NasalCoupling]) -> NasalCoupling:
    base = coupling or NasalCoupling()
    return replace(
        base,
        port_open=_clamp01(base.port_open),
        vowel_leak=_clamp01(base.vowel_leak),
        nostril_area_cm2=max(0.1, float(base.nostril_area_cm2)),
        nasal_cavity_length_cm=max(5.0, float(base.nasal_cavity_length_cm)),
        nasal_cavity_area_cm2=max(0.5, float(base.nasal_cavity_area_cm2)),
        sinus_cavity_length_cm=max(1.0, float(base.sinus_cavity_length_cm)),
        sinus_cavity_area_cm2=max(0.1, float(base.sinus_cavity_area_cm2)),
        sinus_coupling_area_cm2=max(0.05, float(base.sinus_coupling_area_cm2)),
        loss_db_per_meter=max(0.0, float(base.loss_db_per_meter)),
    )


def _nasal_branch_zeros(
    coupling: NasalCoupling,
    sr: int,
    *,
    port_override: Optional[float] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    cfg = _sanitize_nasal_coupling(coupling)
    if port_override is not None:
        cfg = replace(cfg, port_open=_clamp01(port_override))
    port = cfg.port_open
    if sr <= 0 or port <= EPS:
        return np.zeros(0, dtype=np.float64), np.zeros(0, dtype=np.float64)

    eff_len = max(5.0, cfg.nasal_cavity_length_cm + 0.35 * cfg.nostril_area_cm2)
    base_freq = SPEED_OF_SOUND_CM_S / (4.0 * eff_len)
    area_ratio = cfg.nasal_cavity_area_cm2 / max(cfg.nostril_area_cm2, 0.1)
    damping = cfg.loss_db_per_meter * 38.0

    zeros: List[float] = []
    bws: List[float] = []
    for idx in range(3):
        freq = base_freq * (2 * idx + 1)
        if freq >= sr * 0.48:
            break
        q = max(1.8, area_ratio * (1.35 + 1.25 * port))
        bw = freq / q + damping
        zeros.append(freq)
        bws.append(bw)

    if cfg.sinus_coupling_area_cm2 > 0.05 and cfg.sinus_cavity_area_cm2 > 0.05:
        eff_sinus_len = max(
            1.0,
            cfg.sinus_cavity_length_cm
            + 0.18 * (cfg.sinus_cavity_area_cm2 / max(cfg.sinus_coupling_area_cm2, 0.05)),
        )
        sinus_freq = SPEED_OF_SOUND_CM_S / (4.0 * eff_sinus_len)
        if sinus_freq < sr * 0.48:
            q_s = max(
                1.6,
                (cfg.sinus_cavity_area_cm2 / max(cfg.sinus_coupling_area_cm2, 0.05))
                * (1.05 + 0.6 * port),
            )
            bw_s = sinus_freq / q_s + damping * 0.8
            zeros.append(sinus_freq)
            bws.append(bw_s)

    if not zeros:
        return np.zeros(0, dtype=np.float64), np.zeros(0, dtype=np.float64)

    zero_arr = np.asarray(zeros, dtype=np.float64)
    bw_arr = np.asarray(bws, dtype=np.float64)
    order = np.argsort(zero_arr)
    return zero_arr[order], bw_arr[order]


def _estimate_nasal_formants(
    consonant: str,
    coupling: NasalCoupling,
    *,
    port_override: Optional[float] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    cfg = _sanitize_nasal_coupling(coupling)
    if port_override is not None:
        cfg = replace(cfg, port_open=_clamp01(port_override))
    port = cfg.port_open

    nasal_len = max(6.0, cfg.nasal_cavity_length_cm + 0.6 * cfg.nostril_area_cm2)
    base_freq = SPEED_OF_SOUND_CM_S / (4.0 * nasal_len)
    scale = 1.4 + 1.1 * port + 0.03 * cfg.nostril_area_cm2
    f1 = base_freq / scale
    oral_front = 4.3 if consonant == 'n' else 5.6
    oral_mid = 3.1 if consonant == 'n' else 3.9
    scale2 = 2.0 + 0.8 * (1.0 - port) + 0.03 * cfg.nasal_cavity_area_cm2
    scale3 = 2.05 + 0.6 * (1.0 - port) + 0.02 * (
        cfg.nasal_cavity_area_cm2 + cfg.sinus_coupling_area_cm2
    )
    f2 = SPEED_OF_SOUND_CM_S / (2.0 * oral_front * scale2)
    f3 = SPEED_OF_SOUND_CM_S / (2.0 * oral_mid * scale3)

    if consonant == 'm':
        f1 *= 0.92
        f2 *= 0.88
        scale_offset = 1.0 + 0.04 * cfg.nostril_area_cm2
        f3 *= 0.94 / scale_offset

    f1 = float(np.clip(f1, 150.0, 450.0))
    f2 = float(np.clip(f2, 1100.0, 2300.0))
    f3 = float(np.clip(f3, 2000.0, 3200.0))

    loss_term = cfg.loss_db_per_meter * 22.0
    bw1 = 70.0 + 130.0 * port + loss_term
    bw2 = 150.0 + 120.0 * port + loss_term * 0.6
    bw3 = 210.0 + 110.0 * port + loss_term * 0.5
    if consonant == 'm':
        bw1 += 12.0
        bw2 += 8.0

    return (
        np.array([f1, f2, f3], dtype=np.float64),
        np.array([bw1, bw2, bw3], dtype=np.float64),
    )


def _resample_profile(values: np.ndarray, size: int) -> np.ndarray:
    if len(values) == size:
        return values.copy()
    positions = np.linspace(0.0, len(values) - 1.0, num=len(values))
    target = np.linspace(0.0, len(values) - 1.0, num=size)
    return np.interp(target, positions, values).astype(np.float64)


def generate_kelly_lochbaum_profile(
    *,
    numSections: int = DEFAULT_TRACT_SECTIONS,
    articulation: Optional[Dict[str, float]] = None,
    jaw: float = 0.0,
    tongueBody: float = 0.0,
    tongueTip: float = 0.0,
    lipHeight: float = 0.0,
    lipProtrusion: float = 0.0,
    pharynx: float = 0.0,
    smoothing: bool = True,
) -> np.ndarray:
    """Generate a Kelly-Lochbaum area profile from simplified articulation controls."""

    params = {
        'jaw': jaw,
        'tongueBody': tongueBody,
        'tongueTip': tongueTip,
        'lipHeight': lipHeight,
        'lipProtrusion': lipProtrusion,
        'pharynx': pharynx,
    }
    if articulation:
        params.update(articulation)

    params = {k: _clamp(float(v), -1.0, 1.0) for k, v in params.items()}

    base = _resample_profile(_NEUTRAL_TRACT_AREA_CM2, max(2, int(numSections)))
    sections = len(base)
    idx = np.arange(sections, dtype=np.float64)

    areas = base.copy()

    if params['jaw'] != 0.0:
        areas *= np.exp(0.35 * params['jaw'])

    if params['pharynx'] != 0.0:
        weight = np.linspace(1.0, 0.2, sections)
        areas *= np.exp(-0.6 * params['pharynx'] * weight)

    if params['tongueBody'] != 0.0:
        center = sections * (0.45 + 0.1 * params['tongueBody'])
        width = max(1.5, 3.5 - params['tongueBody'] * 1.0)
        bump = np.exp(-0.5 * ((idx - center) / width) ** 2)
        areas *= 1.0 - 0.35 * params['tongueBody'] * bump

    if params['tongueTip'] != 0.0:
        center = sections * 0.72
        width = 2.2
        bump = np.exp(-0.5 * ((idx - center) / width) ** 2)
        areas *= 1.0 - 0.25 * params['tongueTip'] * bump

    if params['lipHeight'] != 0.0:
        weights = np.clip((idx - (sections - 3)) / 3.0, 0.0, 1.0)
        areas *= 1.0 - 0.5 * params['lipHeight'] * weights

    if params['lipProtrusion'] != 0.0:
        protrude = 1.0 - 0.4 * params['lipProtrusion']
        areas[-3:] *= protrude

    areas = np.clip(areas, 0.05, None)

    if smoothing and sections > 2:
        kernel = np.array([0.25, 0.5, 0.25], dtype=np.float64)
        padded = np.pad(areas, (1, 1), mode='edge')
        areas = np.convolve(padded, kernel, mode='valid')

    return areas.astype(np.float64, copy=False)


def _reflection_to_lpc(reflection: Iterable[float]) -> np.ndarray:
    a = np.array([1.0], dtype=np.float64)
    for k in reflection:
        k = float(_clamp(k, -0.999, 0.999))
        m = len(a)
        a_new = np.empty(m + 1, dtype=np.float64)
        a_new[0] = 1.0
        for i in range(1, m):
            a_new[i] = a[i] + k * a[m - i]
        a_new[m] = k
        a = a_new
    return a


def _area_profile_to_reflections(
    area_profile: Sequence[float],
    *,
    lipReflection: float = -0.85,
) -> np.ndarray:
    """Convert a Kelly-Lochbaum area profile to reflection coefficients."""

    areas = np.asarray(area_profile, dtype=np.float64)
    if areas.ndim != 1 or len(areas) < 2:
        raise ValueError('area_profile must be a 1-D sequence with >=2 elements')

    refl: List[float] = []
    for left, right in zip(areas[:-1], areas[1:]):
        denom = left + right
        if denom <= EPS:
            refl.append(0.0)
        else:
            refl.append((right - left) / denom)
    refl.append(float(_clamp(lipReflection, -0.999, 0.0)))
    return np.asarray(refl, dtype=np.float64)


def area_profile_to_formants(
    area_profile: Sequence[float],
    *,
    nFormants: int = 3,
    lipReflection: float = -0.85,
    wallLoss: float = 0.996,
) -> Tuple[np.ndarray, np.ndarray]:
    """Estimate formant frequencies/bandwidths from a Kelly-Lochbaum area profile."""

    areas = np.asarray(area_profile, dtype=np.float64)
    if areas.ndim != 1 or len(areas) < 2:
        raise ValueError('area_profile must be a 1-D sequence with >=2 elements')

    refl = _area_profile_to_reflections(areas, lipReflection=lipReflection)

    a_poly = _reflection_to_lpc(refl)
    roots = np.roots(a_poly)
    roots = roots[np.abs(roots) > EPS]
    roots = roots[np.imag(roots) >= 0.0]

    section_len_cm = VOCAL_TRACT_LENGTH_CM / float(len(areas))
    fs = SPEED_OF_SOUND_CM_S / (2.0 * section_len_cm)

    freq = np.angle(roots) * fs / (2.0 * np.pi)
    damp = np.abs(roots) * wallLoss
    bw = -fs * np.log(np.clip(damp, EPS, 0.9999)) / np.pi

    valid = np.isfinite(freq) & np.isfinite(bw) & (freq > 0.0) & (bw > 0.0)
    freq = freq[valid]
    bw = bw[valid]

    if len(freq) == 0:
        return np.zeros(0, dtype=np.float64), np.zeros(0, dtype=np.float64)

    order = np.argsort(freq)
    freq = freq[order][:nFormants]
    bw = bw[order][:nFormants]
    return freq.astype(np.float64), bw.astype(np.float64)

def _normalize_to_hiragana(text: str) -> str:
    """NFKC normalize and convert katakana to hiragana."""
    normalized = unicodedata.normalize('NFKC', text)
    buf: List[str] = []
    for ch in normalized:
        code = ord(ch)
        if 0x30A1 <= code <= 0x30F3:           # カタカナ → ひらがな
            buf.append(chr(code - 0x60))
        elif ch == 'ヴ':
            buf.append('ゔ')
        elif ch == 'ヵ':
            buf.append('ゕ')
        elif ch == 'ヶ':
            buf.append('ゖ')
        else:
            buf.append(ch)
    return ''.join(buf)


def _append_pause(tokens: List[str]) -> None:
    """トークン列の末尾に休符 'pau' を付与する（既に末尾が休符なら何もしない）。
    
    Args:
        tokens (List[str]): CV/母音/鼻音/休符のトークン列（破壊的に更新）。
    """
    if tokens and tokens[-1] == PAUSE_TOKEN:
        return
    tokens.append(PAUSE_TOKEN)


def _extend_last_vowel(tokens: List[str]) -> None:
    """長音記号の処理：直前の母音を延長（なければ何もしない）"""
    for t in reversed(tokens):
        if t == PAUSE_TOKEN:
            continue
        if t in VOWEL_TABLE:
            tokens.append(t)
            return
        if t in CV_TOKEN_MAP:
            tokens.append(CV_TOKEN_MAP[t][1])
            return
    # fallback: do nothing


def _parse_romaji_sequence(seq: str) -> List[str]:
    """ASCII ローマ字を CV/母音/鼻音トークン列へ分割"""
    tokens: List[str] = []
    i, n = 0, len(seq)
    while i < n:
        matched = False
        max_len = min(_MAX_ROMAJI_TOKEN_LENGTH, n - i)
        for size in range(max_len, 0, -1):
            cand = seq[i:i + size]
            if cand in _VALID_TOKENS:
                tokens.append(cand)
                i += size
                matched = True
                break
        if matched:
            continue
        ch = seq[i]
        if ch in 'aiueo':
            tokens.append(ch)
        elif ch == 'n':
            tokens.append('n')
        i += 1
    return tokens


def text_to_tokens(text: str) -> List[str]:
    """Convert arbitrary text to the synthesizer token sequence."""
    if not text:
        return []

    tokens: List[str] = []
    roma_buf: List[str] = []

    def flush_buf() -> None:
        """ローマ字の一時バッファを解析してトークン列にフラッシュする内部ヘルパー。"""
        nonlocal roma_buf
        if roma_buf:
            tokens.extend(_parse_romaji_sequence(''.join(roma_buf)))
            roma_buf = []

    normalized = _normalize_to_hiragana(text)
    n, pos = len(normalized), 0

    while pos < n:
        ch = normalized[pos]

        # ASCII ローマ字をバッファリング
        if ch.isascii() and ch.isalpha():
            roma_buf.append(ch.lower())
            pos += 1
            continue

        flush_buf()

        # 空白・句読点 → ポーズ
        if ch.isspace() or ch in _PUNCTUATION_CHARS:
            _append_pause(tokens)
            pos += 1
            continue

        # 長音記号
        if ch == 'ー':
            _extend_last_vowel(tokens)
            pos += 1
            continue

        # 促音（簡易：無音ポーズ）
        if ch == 'っ':
            _append_pause(tokens)
            pos += 1
            continue

        # 拗音二文字
        nxt = normalized[pos + 1] if pos + 1 < n else ''
        pair = ch + nxt
        if pair in _KANA_DIGRAPH_MAP:
            tokens.extend(_KANA_DIGRAPH_MAP[pair])
            pos += 2
            continue

        # 単音
        if ch in _KANA_BASE_MAP:
            tokens.extend(_KANA_BASE_MAP[ch])
            pos += 1
            continue

        # 濁音・半濁音
        if ch in _VOICED_KANA_MAP:
            base = _VOICED_KANA_MAP[ch]
            tokens.extend(_KANA_BASE_MAP.get(base, []))
            pos += 1
            continue
        if ch in _HAND_DAKUTEN_MAP:
            base = _HAND_DAKUTEN_MAP[ch]
            tokens.extend(_KANA_BASE_MAP.get(base, []))
            pos += 1
            continue

        # 数字 → 区切り
        if ch.isdigit():
            _append_pause(tokens)
            pos += 1
            continue

        # 未対応はスキップ
        pos += 1

    flush_buf()

    # 圧縮: 連続ポーズ削除／先頭末尾ポーズ除去
    out: List[str] = []
    for t in tokens:
        if t == PAUSE_TOKEN and (not out or out[-1] == PAUSE_TOKEN):
            continue
        out.append(t)
    if out and out[0] == PAUSE_TOKEN:
        out = out[1:]
    if out and out[-1] == PAUSE_TOKEN:
        out = out[:-1]
    return out

# =======================
# Utilities (numeric)
# =======================

def _ms_to_samples(ms: float, sr: int) -> int:
    """ミリ秒をサンプル数に変換（切り捨て・下限0）"""
    if ms <= 0:
        return 0
    return int(sr * (ms / 1000.0))


def _db_to_lin(db: float) -> float:
    """dB → 線形ゲイン"""
    return 10.0 ** (db / 20.0)


def _ensure_array(x: np.ndarray, *, dtype=DTYPE) -> np.ndarray:
    """型・連続性を整える"""
    if x.dtype != dtype:
        return x.astype(dtype, copy=False)
    return x


def _normalize_peak(sig: np.ndarray, target: float = PEAK_DEFAULT) -> np.ndarray:
    """ピーク正規化（無音ガード）"""
    sig = _ensure_array(sig)
    peak = float(np.max(np.abs(sig)) + EPS)
    scale = target / peak if peak > 0 else 1.0
    return (sig * scale).astype(DTYPE, copy=False)


def _apply_fade(sig: np.ndarray, sr: int, *, attack_ms: float = 5.0, release_ms: float = 8.0) -> np.ndarray:
    """線形アタック／リリース"""
    sig = _ensure_array(sig)
    n = len(sig)
    if n == 0:
        return sig
    out = sig.copy()

    a = _ms_to_samples(attack_ms, sr)
    r = _ms_to_samples(release_ms, sr)
    if 0 < a < n:
        out[:a] *= np.linspace(0.0, 1.0, a, dtype=DTYPE)
    if 0 < r < n:
        out[-r:] *= np.linspace(1.0, 0.0, r, dtype=DTYPE)
    return out


def _add_breath_noise(sig: np.ndarray, level_db: float) -> np.ndarray:
    """息成分を加算（level_db<0で適用）"""
    rng = np.random.default_rng()
    sig = _ensure_array(sig)
    if level_db >= 0 or len(sig) == 0:
        return sig
    noise = rng.standard_normal(len(sig)).astype(DTYPE)
    rms = float(np.sqrt(np.mean(sig * sig) + EPS))
    target = rms * _db_to_lin(level_db)
    n_rms = float(np.sqrt(np.mean(noise * noise) + EPS))
    if n_rms > 0:
        sig = sig + noise * (target / n_rms)
    return sig.astype(DTYPE, copy=False)

# =======================
# Filters
# =======================

def _bandpass_biquad_coeff(f0: float, Q: float, sr: int) -> Tuple[float, float, float, float, float]:
    """RBJ系BPF係数（ピークゲイン一定タイプ）"""
    w0 = 2.0 * pi * f0 / sr
    alpha = sin(w0) / (2.0 * max(Q, 0.5))
    b0 = alpha
    b1 = 0.0
    b2 = -alpha
    a0 = 1.0 + alpha
    a1 = -2.0 * cos(w0)
    a2 = 1.0 - alpha
    return (b0 / a0, b1 / a0, b2 / a0, a1 / a0, a2 / a0)


def _notch_biquad_coeff(f0: float, bw: float, sr: int) -> Tuple[float, float, float, float, float]:
    """Design a biquad notch filter centred at ``f0`` with bandwidth ``bw``."""

    f0 = float(f0)
    bw = float(max(bw, 1.0))
    if sr <= 0:
        return 1.0, 0.0, 0.0, 0.0, 0.0
    theta = 2.0 * pi * f0 / sr
    theta = float(np.clip(theta, 1e-4, np.pi - 1e-4))
    r = float(np.exp(-pi * bw / sr))
    r = float(_clamp(r, 0.0, 0.9995))
    b0 = 1.0
    b1 = -2.0 * cos(theta)
    b2 = 1.0
    a1 = -2.0 * r * cos(theta)
    a2 = r * r

    sum_b = b0 + b1 + b2
    sum_a = 1.0 + a1 + a2
    if abs(sum_b) > EPS:
        gain = sum_a / sum_b
        b0 *= gain
        b1 *= gain
        b2 *= gain
    return b0, b1, b2, a1, a2


def _biquad_process(x: np.ndarray, b0: float, b1: float, b2: float, a1: float, a2: float) -> np.ndarray:
    """単一Biquadフィルタ"""
    x = _ensure_array(x)
    y = np.empty_like(x)
    x1 = x2 = y1 = y2 = 0.0
    for i, xi in enumerate(x):
        yi = b0 * xi + b1 * x1 + b2 * x2 - a1 * y1 - a2 * y2
        y[i] = yi
        x2, x1 = x1, xi
        y2, y1 = y1, yi
    return y


def _one_pole_lp(x: np.ndarray, cutoff: float, sr: int) -> np.ndarray:
    """一次ローパス"""
    x = _ensure_array(x)
    if len(x) == 0:
        return x
    decay = exp(-2.0 * pi * cutoff / sr)
    y = np.empty_like(x)
    s = 0.0
    for i, xi in enumerate(x):
        s = (1 - decay) * xi + decay * s
        y[i] = s
    return y


def _lip_radiation(x: np.ndarray) -> np.ndarray:
    """唇放射の近似：微分"""
    x = _ensure_array(x)
    n = len(x)
    if n == 0:
        return x
    y = np.empty_like(x)
    y[0] = x[0]
    y[1:] = x[1:] - x[:-1]
    return y

# =======================
# Sources / Noise
# =======================

def _glottal_source(
    f0: float,
    dur_s: float,
    sr: int,
    jitter_cents: float = 10.0,
    shimmer_db: float = 0.8,
) -> np.ndarray:
    """鋸波＋jitter/shimmer。位相加算で生成"""
    rng = np.random.default_rng()
    n = max(0, int(dur_s * sr))
    if n == 0:
        return np.zeros(0, dtype=DTYPE)

    # jitter（AR一次）
    jn = rng.standard_normal(n).astype(DTYPE)
    pole = 0.999
    js = np.empty_like(jn)
    acc = 0.0
    for i in range(n):
        acc = pole * acc + (1.0 - pole) * jn[i]
        js[i] = acc
    cents = (jitter_cents / 100.0) * js
    inst_f = f0 * (2.0 ** (cents / 12.0))

    # 位相加算で鋸波
    phase = np.cumsum(2.0 * pi * inst_f / sr, dtype=np.float64)
    saw = (2.0 * ((phase / (2.0 * pi)) % 1.0) - 1.0).astype(DTYPE)

    # shimmer（AR一次）
    sn = rng.standard_normal(n).astype(DTYPE)
    ss = np.empty_like(sn)
    acc = 0.0
    for i in range(n):
        acc = pole * acc + (1.0 - pole) * sn[i]
        ss[i] = acc
    amp = (_db_to_lin(shimmer_db) ** ss).astype(DTYPE)  # 10**((dB/20)*noise) と等価形

    raw = saw * amp

    # 簡易 HP 抑制
    decay = exp(-2.0 * pi * 800.0 / sr)
    y = np.empty_like(raw)
    s = 0.0
    for i in range(n):
        s = (1 - decay) * raw[i] + decay * s
        y[i] = s
    return y


def _gen_band_noise(
    dur_s: float,
    sr: int,
    center: float,
    Q: float,
    *,
    use_lip_radiation: bool = True,
) -> np.ndarray:
    """帯域ノイズ生成"""
    rng = np.random.default_rng()
    n = max(0, int(dur_s * sr))
    if n == 0:
        return np.zeros(0, dtype=DTYPE)
    noise = rng.standard_normal(n).astype(DTYPE)
    b0, b1, b2, a1, a2 = _bandpass_biquad_coeff(center, Q, sr)
    y = _biquad_process(noise, b0, b1, b2, a1, a2)
    return _lip_radiation(y) if use_lip_radiation else y

# =======================
# Core Synthesis
# =======================

def _apply_formant_filters(src: np.ndarray, formants: Sequence[float], bws: Sequence[float], sr: int) -> np.ndarray:
    """フォルマント設定に基づいて帯域通過を連結"""
    out = np.zeros_like(src, dtype=DTYPE)
    for f, bw in zip(formants, bws):
        Q = max(0.5, float(f) / float(bw))
        b0, b1, b2, a1, a2 = _bandpass_biquad_coeff(float(f), Q, sr)
        out += _biquad_process(src, b0, b1, b2, a1, a2)
    return _lip_radiation(out)


def _apply_nasal_antiresonances(
    src: np.ndarray,
    zero_freqs: Sequence[float],
    zero_bws: Sequence[float],
    sr: int,
    *,
    depth: float = 1.0,
) -> np.ndarray:
    """Apply notch filters that emulate nasal anti-resonances."""

    src = _ensure_array(src)
    if len(src) == 0:
        return src
    if zero_freqs is None or zero_bws is None:
        return src

    zero_freqs = np.asarray(zero_freqs).ravel()
    zero_bws = np.asarray(zero_bws).ravel()

    if zero_freqs.size == 0 or zero_bws.size == 0:
        return src

    depth = _clamp(depth, 0.0, 1.0)
    if depth <= EPS:
        return src

    filtered = src.copy()
    for fz, bw in zip(zero_freqs, zero_bws):
        if not np.isfinite(fz) or not np.isfinite(bw):
            continue
        if fz <= 0.0 or fz >= sr * 0.48:
            continue
        b0, b1, b2, a1, a2 = _notch_biquad_coeff(fz, bw, sr)
        filtered = _biquad_process(filtered, b0, b1, b2, a1, a2)

    if depth >= 1.0:
        return filtered.astype(DTYPE, copy=False)
    mixed = (1.0 - depth) * src + depth * filtered
    return mixed.astype(DTYPE, copy=False)


def _apply_all_pole_filter(src: np.ndarray, a_coeffs: np.ndarray) -> np.ndarray:
    """Apply an all-pole filter defined by ``a_coeffs`` (a[0] == 1)."""

    x = np.asarray(src, dtype=np.float64)
    if x.ndim != 1:
        raise ValueError('src must be one-dimensional')

    order = len(a_coeffs) - 1
    if order <= 0:
        return x.astype(DTYPE, copy=False)

    y = np.empty_like(x, dtype=np.float64)
    for n in range(len(x)):
        acc = x[n]
        for k in range(1, order + 1):
            if n - k >= 0:
                acc -= a_coeffs[k] * y[n - k]
        y[n] = acc
    return y.astype(DTYPE, copy=False)


def _apply_kelly_lochbaum_filter(
    src: np.ndarray,
    area_profile: Sequence[float],
    sr: int,
    *,
    lipReflection: float = -0.85,
    wallLoss: float = 0.996,
    applyRadiation: bool = True,
) -> np.ndarray:
    """Filter ``src`` via a Kelly-Lochbaum (two-port) vocal-tract model."""

    if len(src) == 0:
        return np.zeros(0, dtype=DTYPE)

    refl = _area_profile_to_reflections(area_profile, lipReflection=lipReflection)
    if wallLoss != 1.0:
        refl = np.clip(refl * float(wallLoss), -0.999, 0.999)

    a_poly = _reflection_to_lpc(refl)
    y = _apply_all_pole_filter(src, a_poly)
    if applyRadiation:
        y = _lip_radiation(y)
    return y


def synth_vowel(
    vowel: str = 'a',
    f0: float = 120.0,
    durationSeconds: float = 1.0,
    sampleRate: int = 22050,
    jitterCents: float = 6.0,
    shimmerDb: float = 0.6,
    breathLevelDb: float = -40.0,
    *,
    kellyBlend: Optional[float] = None,
    articulation: Optional[Dict[str, float]] = None,
    areaProfile: Optional[Sequence[float]] = None,
    kellySections: Optional[int] = None,
    useLegacyFormantFilter: bool = True,
    waveguideLipReflection: float = -0.85,
    waveguideWallLoss: float = 0.996,
    speakerProfile: Optional[SpeakerProfile] = None,
) -> np.ndarray:
    """母音合成"""
    assert vowel in VOWEL_TABLE, f"unsupported vowel: {vowel}"
    src = _glottal_source(f0, durationSeconds, sampleRate, jitterCents, shimmerDb)
    spec = VOWEL_TABLE[vowel]

    if kellyBlend is None:
        blend = 1.0 if (articulation is not None or areaProfile is not None) else 0.0
    else:
        blend = float(kellyBlend)
    blend = _clamp(blend, 0.0, 1.0)
    formants = np.array(spec['F'], dtype=np.float64)
    bws = np.array(spec['BW'], dtype=np.float64)

    nasal_zeros: Tuple[np.ndarray, np.ndarray] = (np.zeros(0, dtype=np.float64), np.zeros(0, dtype=np.float64))
    nasal_leak_depth = 0.0
    if speakerProfile is not None:
        coupling = _sanitize_nasal_coupling(speakerProfile.nasal_coupling)
        leak = coupling.vowel_leak
        if leak > EPS:
            base_port = coupling.port_open
            effective_port = leak if base_port <= EPS else leak * base_port
            zeros, zero_bw = _nasal_branch_zeros(coupling, sampleRate, port_override=effective_port)
            if len(zeros) > 0:
                nasal_zeros = (zeros, zero_bw)
                nasal_leak_depth = _clamp(leak * (0.7 + 0.3 * base_port), 0.0, 1.0)

    k_sections = int(kellySections) if kellySections else len(_NEUTRAL_TRACT_AREA_CM2)
    profile_custom: Optional[np.ndarray] = None
    if areaProfile is not None:
        profile_custom = np.asarray(areaProfile, dtype=np.float64)
        if profile_custom.ndim != 1:
            raise ValueError('areaProfile must be one-dimensional')
        if k_sections and len(profile_custom) != k_sections:
            profile_custom = _resample_profile(profile_custom.astype(np.float64), k_sections)
    elif articulation is not None:
        profile_custom = generate_kelly_lochbaum_profile(numSections=k_sections, articulation=articulation or {})

    new_formants: Optional[np.ndarray] = None
    new_bw: Optional[np.ndarray] = None
    if profile_custom is not None:
        try:
            new_formants, new_bw = area_profile_to_formants(
                profile_custom,
                nFormants=len(formants),
                lipReflection=waveguideLipReflection,
                wallLoss=waveguideWallLoss,
            )
        except Exception:
            new_formants, new_bw = None, None

    if new_formants is not None and len(new_formants) > 0 and new_bw is not None:
        if len(new_formants) < len(formants):
            pad = len(formants) - len(new_formants)
            new_formants = np.pad(new_formants, (0, pad), mode='edge')
            new_bw = np.pad(new_bw, (0, pad), mode='edge')
        elif len(new_formants) > len(formants):
            new_formants = new_formants[:len(formants)]
            new_bw = new_bw[:len(formants)]
        formants = (1.0 - blend) * formants + blend * new_formants
        bws = (1.0 - blend) * bws + blend * new_bw

    if useLegacyFormantFilter:
        y = _apply_formant_filters(src, formants, bws, sampleRate)
    else:
        neutral_profile: Optional[np.ndarray] = None

        def _neutral_profile() -> np.ndarray:
            nonlocal neutral_profile
            if neutral_profile is None:
                neutral_profile = generate_kelly_lochbaum_profile(numSections=k_sections, articulation={})
            return neutral_profile

        if profile_custom is None:
            profile_waveguide = _neutral_profile()
        else:
            if blend <= 0.0:
                profile_waveguide = _neutral_profile()
            elif blend >= 1.0:
                profile_waveguide = profile_custom
            else:
                profile_waveguide = np.clip((1.0 - blend) * _neutral_profile() + blend * profile_custom, 0.05, None)
        y = _apply_kelly_lochbaum_filter(
            src,
            profile_waveguide,
            sampleRate,
            lipReflection=waveguideLipReflection,
            wallLoss=waveguideWallLoss,
            applyRadiation=True,
        )
    y = _add_breath_noise(y, breathLevelDb)
    if nasal_leak_depth > EPS and nasal_zeros[0].size > 0:
        y = _apply_nasal_antiresonances(
            y,
            nasal_zeros[0],
            nasal_zeros[1],
            sampleRate,
            depth=nasal_leak_depth,
        )
    return _normalize_peak(y, PEAK_DEFAULT)


def synth_fricative(
    consonant: str = 's',
    durationSeconds: float = 0.16,
    sampleRate: int = 22050,
    levelDb: float = -12.0,
) -> np.ndarray:
    """無声摩擦音の簡易実装。"""
    c = consonant.lower()
    if c == 's':
        center, Q, lip, lp = 6500.0, 3.0, True, None
    elif c == 'sh':
        center, Q, lip, lp = 3800.0, 2.4, True, 4200.0
    elif c == 'h':
        center, Q, lip, lp = 1800.0, 1.4, False, 2300.0
    elif c == 'f':
        center, Q, lip, lp = 950.0, 1.3, False, 2100.0
    else:
        raise ValueError("synth_fricative: supported consonants are 's','sh','h','f'.")

    y = _gen_band_noise(durationSeconds, sampleRate, center, Q, use_lip_radiation=lip)
    if lp is not None:
        y = _one_pole_lp(y, cutoff=lp, sr=sampleRate)
    if c == 'h':
        y = _one_pole_lp(y, cutoff=1700.0, sr=sampleRate)

    attack = 6 if c in ('h', 'f') else 4
    release = 16 if c in ('h', 'f') else 12
    y = _apply_fade(y, sampleRate, attack_ms=attack, release_ms=release)

    peak = 0.6 if c not in ('h', 'f') else (0.5 if c == 'h' else 0.55)
    y = _normalize_peak(y, peak)
    gain = _db_to_lin(levelDb)
    if gain != 1.0:
        y = (y * gain).astype(DTYPE, copy=False)
    return y


def synth_plosive(
    consonant: str = 't',
    sampleRate: int = 22050,
    closureMilliseconds: Optional[float] = None,
    burstMilliseconds: Optional[float] = None,
    aspirationMilliseconds: Optional[float] = None,
    levelDb: float = -10.0,
) -> np.ndarray:
    """無声破裂音（t / k）"""
    c = consonant.lower()
    if c not in ('t', 'k'):
        raise ValueError("synth_plosive: supported only 't' or 'k'.")

    if c == 't':
        closure = 24 if closureMilliseconds is None else closureMilliseconds
        burst = 12 if burstMilliseconds is None else burstMilliseconds
        asp = 18 if aspirationMilliseconds is None else aspirationMilliseconds
        b_center, b_Q = 4500.0, 1.2
        a_center, a_Q = 6000.0, 0.9
    else:
        closure = 32 if closureMilliseconds is None else closureMilliseconds
        burst = 14 if burstMilliseconds is None else burstMilliseconds
        asp = 24 if aspirationMilliseconds is None else aspirationMilliseconds
        b_center, b_Q = 1700.0, 1.2
        a_center, a_Q = 2500.0, 0.9

    # 閉鎖
    closure_seg = np.zeros(_ms_to_samples(closure, sampleRate), dtype=DTYPE)

    # 破裂
    burst_len = _ms_to_samples(burst, sampleRate)
    burst_noise = _gen_band_noise(burst_len / sampleRate, sampleRate, b_center, b_Q, use_lip_radiation=False)
    tau = max(1.0, burst_len / 4.0)
    env = np.exp(-np.arange(burst_len, dtype=DTYPE) / tau)
    burst_noise = burst_noise[:burst_len] * env

    # 後続の息
    asp_len = _ms_to_samples(asp, sampleRate)
    asp_noise = _gen_band_noise(asp_len / sampleRate, sampleRate, a_center, a_Q, use_lip_radiation=False)
    asp_noise = _apply_fade(asp_noise, sampleRate, attack_ms=3, release_ms=18)
    asp_noise = _one_pole_lp(asp_noise, cutoff=4500.0, sr=sampleRate)

    y = np.concatenate([closure_seg, burst_noise, asp_noise]).astype(DTYPE)
    y *= _db_to_lin(levelDb)
    return _normalize_peak(y, 0.6)


def _crossfade(a: np.ndarray, b: np.ndarray, sr: int, *, overlap_ms: float = 30.0) -> np.ndarray:
    """a → b をオーバーラップ結合"""
    a = _ensure_array(a)
    b = _ensure_array(b)
    ov = min(_ms_to_samples(overlap_ms, sr), len(a), len(b))
    if ov <= 0:
        return np.concatenate([a, b]).astype(DTYPE)

    fade_out = np.linspace(1.0, 0.0, ov, dtype=DTYPE)
    fade_in = 1.0 - fade_out
    head = a[:-ov]
    tail = a[-ov:] * fade_out + b[:ov] * fade_in
    rest = b[ov:]
    return np.concatenate([head, tail, rest]).astype(DTYPE)


def _synth_vowel_fixed(
    formants: Sequence[float],
    bws: Sequence[float],
    f0: float,
    dur_s: float,
    sr: int,
    jitterCents: float = 6.0,
    shimmerDb: float = 0.6,
    breathLevelDb: float = -40.0,
    *,
    areaProfile: Optional[Sequence[float]] = None,
    articulation: Optional[Dict[str, float]] = None,
    kellySections: Optional[int] = None,
    useLegacyFormantFilter: bool = True,
    waveguideLipReflection: float = -0.85,
    waveguideWallLoss: float = 0.996,
) -> np.ndarray:
    """与えたフォルマントで固定合成（短区間）"""
    src = _glottal_source(f0, dur_s, sr, jitterCents, shimmerDb)
    if useLegacyFormantFilter:
        y = _apply_formant_filters(src, formants, bws, sr)
    else:
        sections = int(kellySections) if kellySections else len(_NEUTRAL_TRACT_AREA_CM2)
        profile: Optional[np.ndarray]
        if areaProfile is not None:
            profile = np.asarray(areaProfile, dtype=np.float64)
            if profile.ndim != 1:
                raise ValueError('areaProfile must be one-dimensional')
            if sections and len(profile) != sections:
                profile = _resample_profile(profile.astype(np.float64), sections)
        elif articulation is not None:
            profile = generate_kelly_lochbaum_profile(numSections=sections, articulation=articulation or {})
        else:
            profile = generate_kelly_lochbaum_profile(numSections=sections, articulation={})
        y = _apply_kelly_lochbaum_filter(
            src,
            profile,
            sr,
            lipReflection=waveguideLipReflection,
            wallLoss=waveguideWallLoss,
            applyRadiation=True,
        )
    y = _add_breath_noise(y, breathLevelDb)
    return _normalize_peak(y, PEAK_DEFAULT)


def _pre_emphasis(x: np.ndarray, coefficient: float = 0.85) -> np.ndarray:
    """y[n] = x[n] - a*x[n-1]（高域をわずかに強調）"""
    x = _ensure_array(x)
    n = len(x)
    if n == 0:
        return x
    y = np.empty_like(x)
    y[0] = x[0]
    y[1:] = x[1:] - coefficient * x[:-1]
    return y


def synth_vowel_with_onset(
    vowel: str,
    f0: float,
    sampleRate: int,
    totalMilliseconds: int = 240,
    onsetMilliseconds: int = 45,
    onsetFormants: Optional[Sequence[float]] = None,
    onsetBandwidthScale: float = 0.85,
    vowelModel: Optional[Dict[str, Any]] = None,
) -> np.ndarray:
    """母音先頭だけフォルマント遷移を与える簡易版"""
    spec = VOWEL_TABLE[vowel]
    targetF, targetBW = spec['F'], spec['BW']
    vowel_kwargs = dict(vowelModel or {})
    waveguide_opts = {
        'areaProfile': vowel_kwargs.get('areaProfile'),
        'articulation': vowel_kwargs.get('articulation'),
        'kellySections': vowel_kwargs.get('kellySections'),
        'useLegacyFormantFilter': vowel_kwargs.get('useLegacyFormantFilter', True),
        'waveguideLipReflection': vowel_kwargs.get('waveguideLipReflection', -0.85),
        'waveguideWallLoss': vowel_kwargs.get('waveguideWallLoss', 0.996),
    }

    if not onsetFormants:
        return synth_vowel(
            vowel=vowel,
            f0=f0,
            durationSeconds=totalMilliseconds / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )

    total_ms = int(max(10, totalMilliseconds))
    onset_ms = int(max(1, min(onsetMilliseconds, total_ms - 1)))
    sustain_ms = max(0, total_ms - onset_ms)

    onsetBW = [bw * float(onsetBandwidthScale) for bw in targetBW]
    onset = _synth_vowel_fixed(
        onsetFormants,
        onsetBW,
        f0,
        onset_ms / 1000.0,
        sampleRate,
        **waveguide_opts,
    )
    onset = _apply_fade(onset, sampleRate, attack_ms=6.0, release_ms=min(12.0, onset_ms * 0.5))

    if sustain_ms <= 0:
        return onset

    ov_ms = min(max(6.0, onset_ms * 0.45), 14.0, float(sustain_ms))
    rest_len_ms = sustain_ms + max(0.0, ov_ms)

    if vowel_kwargs:
        sustain = synth_vowel(
            vowel=vowel,
            f0=f0,
            durationSeconds=rest_len_ms / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )
    else:
        sustain = _synth_vowel_fixed(
            targetF,
            targetBW,
            f0,
            rest_len_ms / 1000.0,
            sampleRate,
            **waveguide_opts,
        )
    sustain = _apply_fade(sustain, sampleRate, attack_ms=4.0, release_ms=12.0)

    return _crossfade(onset, sustain, sampleRate, overlap_ms=ov_ms)

# ===== Glide/Liquid onsets =====

GLIDE_ONSETS: Dict[str, Dict[str, Sequence[float]]] = {
    'w': {
        'a': [420.0, 1000.0, 2300.0],
        'i': [400.0, 1200.0, 2500.0],
        'u': [380.0,  900.0, 2200.0],
        'e': [410.0, 1100.0, 2350.0],
        'o': [420.0,  950.0, 2300.0],
    },
    'y': {
        'a': [380.0, 2600.0, 3300.0],
        'i': [360.0, 2900.0, 3500.0],
        'u': [370.0, 2650.0, 3300.0],
        'e': [380.0, 2800.0, 3400.0],
        'o': [380.0, 2500.0, 3200.0],
    },
}

LIQUID_ONSETS: Dict[str, Sequence[float]] = {
    'a': [480.0, 1350.0, 2100.0],
    'i': [380.0, 1800.0, 2200.0],
    'u': [420.0, 1500.0, 2100.0],
    'e': [430.0, 1600.0, 2200.0],
    'o': [460.0, 1400.0, 2150.0],
}

# =======================
# Composition (CV/フレーズ)
# =======================

def synth_affricate(
    consonant: str = 'ch',
    sampleRate: int = 22050,
    closureMilliseconds: Optional[float] = None,
    fricativeMilliseconds: Optional[float] = None,
    levelDb: float = -11.0,
) -> np.ndarray:
    """破擦音 /ch/, /ts/ の簡易版"""
    c = consonant.lower()
    if c not in ('ch', 'ts'):
        raise ValueError("synth_affricate: supported only 'ch' or 'ts'.")

    if c == 'ch':
        closure = 26 if closureMilliseconds is None else closureMilliseconds
        burst_ms = 10
        fric_ms = 95.0 if fricativeMilliseconds is None else fricativeMilliseconds
        fric = synth_fricative('sh', durationSeconds=fric_ms / 1000.0, sampleRate=sampleRate, levelDb=-16.0)
    else:
        closure = 24 if closureMilliseconds is None else closureMilliseconds
        burst_ms = 9
        fric_ms = 90.0 if fricativeMilliseconds is None else fricativeMilliseconds
        fric = synth_fricative('s', durationSeconds=fric_ms / 1000.0, sampleRate=sampleRate, levelDb=-16.0)

    plosive = synth_plosive(
        't',
        sampleRate=sampleRate,
        closureMilliseconds=closure,
        burstMilliseconds=burst_ms,
        aspirationMilliseconds=0.0,
        levelDb=levelDb,
    )
    y = _crossfade(plosive, fric, sampleRate, overlap_ms=14)
    return _normalize_peak(y, 0.6)


NASAL_PRESETS: Dict[str, Dict[str, Sequence[float]]] = {
    'n': {'F': [250.0, 1900.0, 2800.0], 'BW': [80.0, 160.0, 220.0]},
    'm': {'F': [220.0, 1600.0, 2500.0], 'BW': [70.0, 150.0, 210.0]},
}


def synth_nasal(
    consonant: str = 'n',
    f0: float = 120.0,
    durationMilliseconds: float = 90.0,
    sampleRate: int = 22050,
    *,
    nasalCoupling: Optional[NasalCoupling] = None,
    speakerProfile: Optional[SpeakerProfile] = None,
    portOpen: Optional[float] = None,
    nostrilAreaCm2: Optional[float] = None,
    breathLevelDb: float = -38.0,
) -> np.ndarray:
    """簡易的な鼻音 /n/, /m/"""
    c = consonant.lower()
    if c not in NASAL_PRESETS:
        raise ValueError("synth_nasal: supported nasals are 'n' or 'm'.")

    dur_s = max(20.0, float(durationMilliseconds)) / 1000.0
    coupling = nasalCoupling
    if coupling is None and speakerProfile is not None:
        coupling = speakerProfile.nasal_coupling
    coupling = _sanitize_nasal_coupling(coupling)
    if portOpen is not None:
        coupling = replace(coupling, port_open=_clamp01(portOpen))
    if nostrilAreaCm2 is not None:
        coupling = replace(coupling, nostril_area_cm2=max(0.1, float(nostrilAreaCm2)))

    formants, bws = _estimate_nasal_formants(c, coupling)
    zeros, zero_bw = _nasal_branch_zeros(coupling, sampleRate)

    breath_level = float(breathLevelDb)
    breath_level += (1.0 - coupling.port_open) * 6.0
    y = _synth_vowel_fixed(
        formants,
        bws,
        f0,
        dur_s,
        sampleRate,
        jitterCents=4.0,
        shimmerDb=0.4,
        breathLevelDb=breath_level,
    )
    depth = _clamp(0.75 + 0.2 * coupling.port_open, 0.0, 1.0)
    if zeros.size > 0:
        y = _apply_nasal_antiresonances(y, zeros, zero_bw, sampleRate, depth=depth)

    y = _apply_fade(
        y,
        sampleRate,
        attack_ms=8.0 if c == 'n' else 10.0,
        release_ms=22.0 if c == 'n' else 28.0,
    )
    gain = 0.6 if c == 'n' else 0.68
    gain += 0.06 * (coupling.port_open - 0.8)
    gain = _clamp(gain, 0.45, 0.82)
    return _normalize_peak(y * gain, 0.5)


def synth_cv(
    cons: str,
    vowel: str,
    f0: float = 120.0,
    sampleRate: int = 22050,
    preMilliseconds: int = 0,
    consonantMilliseconds: Optional[int] = None,
    vowelMilliseconds: int = 240,
    overlapMilliseconds: int = 30,
    useOnsetTransition: bool = False,
    vowelModel: Optional[Dict[str, Any]] = None,
    speakerProfile: Optional[SpeakerProfile] = None,
) -> np.ndarray:
    """子音 + 母音（50音相当までカバー）"""
    c = cons.lower()
    v = vowel.lower()
    if v not in VOWEL_TABLE:
        raise ValueError("Unknown vowel for synth_cv")

    head = np.zeros(_ms_to_samples(preMilliseconds, sampleRate), dtype=DTYPE)
    vowel_kwargs = dict(vowelModel or {})
    if speakerProfile is not None:
        vowel_kwargs.setdefault('speakerProfile', speakerProfile)
    speaker_profile: Optional[SpeakerProfile] = vowel_kwargs.get('speakerProfile')
    nasal_coupling = speaker_profile.nasal_coupling if speaker_profile is not None else None

    if c in ('s', 'sh'):
        level = {'s': -14.0, 'sh': -15.0}[c]
        default_ms = {'s': 160.0, 'sh': 150.0}[c]
        fric_ms = float(consonantMilliseconds) if consonantMilliseconds is not None else default_ms
        fric = synth_fricative(c, durationSeconds=fric_ms / 1000.0, sampleRate=sampleRate, levelDb=level)
        vow = synth_vowel(
            vowel=v,
            f0=f0,
            durationSeconds=vowelMilliseconds / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )
        out = _crossfade(fric, vow, sampleRate, overlap_ms=max(24, overlapMilliseconds))

    elif c in ('h', 'f'):
        default_ms = {'h': 190.0, 'f': 170.0}[c]
        cons_len = float(consonantMilliseconds) if consonantMilliseconds is not None else default_ms
        cons_len = max(0.0, cons_len)
        sil = np.zeros(_ms_to_samples(cons_len, sampleRate), dtype=DTYPE)
        vow = synth_vowel(
            vowel=v,
            f0=f0,
            durationSeconds=vowelMilliseconds / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )

        req_ov = float(overlapMilliseconds) if consonantMilliseconds is not None else 12.0
        eff_ov = min(12.0, cons_len, max(0.0, req_ov))
        out = _crossfade(sil, vow, sampleRate, overlap_ms=eff_ov) if eff_ov > 0 else np.concatenate([sil, vow]).astype(DTYPE)
        out = _apply_fade(out, sampleRate, attack_ms=8, release_ms=12)

    elif c in ('t', 'k'):
        plo = synth_plosive(c, sampleRate=sampleRate)
        if useOnsetTransition and v == 'a':
            onsetF = [800, 1800, 3000] if c == 't' else [800, 2200, 2400]
            vow = synth_vowel_with_onset(
                'a',
                f0,
                sampleRate,
                totalMilliseconds=vowelMilliseconds,
                onsetMilliseconds=45,
                onsetFormants=onsetF,
                vowelModel=vowel_kwargs,
            )
        else:
            vow = synth_vowel(
                vowel=v,
                f0=f0,
                durationSeconds=vowelMilliseconds / 1000.0,
                sampleRate=sampleRate,
                **vowel_kwargs,
            )

        try:
            req = int(overlapMilliseconds)
            ov = max(6, min(req, 12))
        except Exception:
            ov = 8 if c == 't' else 10
        out = _crossfade(plo, vow, sampleRate, overlap_ms=ov)

    elif c in ('ch', 'ts'):
        aff = synth_affricate(c, sampleRate=sampleRate)
        vow = synth_vowel(
            vowel=v,
            f0=f0,
            durationSeconds=vowelMilliseconds / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )
        out = _crossfade(aff, vow, sampleRate, overlap_ms=max(12, min(overlapMilliseconds, 18)))

    elif c == 'n':
        nasal_ms = float(consonantMilliseconds) if consonantMilliseconds is not None else 90.0
        nasal = synth_nasal(
            'n',
            f0=f0,
            durationMilliseconds=nasal_ms,
            sampleRate=sampleRate,
            nasalCoupling=nasal_coupling,
            speakerProfile=speaker_profile,
        )
        vow = synth_vowel(
            vowel=v,
            f0=f0,
            durationSeconds=vowelMilliseconds / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )
        out = _crossfade(nasal, vow, sampleRate, overlap_ms=max(25, overlapMilliseconds))

    elif c == 'm':
        nasal_ms = float(consonantMilliseconds) if consonantMilliseconds is not None else 110.0
        nasal = synth_nasal(
            'm',
            f0=f0,
            durationMilliseconds=nasal_ms,
            sampleRate=sampleRate,
            nasalCoupling=nasal_coupling,
            speakerProfile=speaker_profile,
        )
        vow = synth_vowel(
            vowel=v,
            f0=f0,
            durationSeconds=vowelMilliseconds / 1000.0,
            sampleRate=sampleRate,
            **vowel_kwargs,
        )
        out = _crossfade(nasal, vow, sampleRate, overlap_ms=max(28, overlapMilliseconds))

    elif c == 'w':
        onset_ms = float(consonantMilliseconds) if consonantMilliseconds is not None else 44.0
        onset_ms = max(24.0, min(onset_ms, float(vowelMilliseconds) - 8.0))
        onsetF = GLIDE_ONSETS['w'].get(v, GLIDE_ONSETS['w']['a'])
        out = synth_vowel_with_onset(
            v,
            f0,
            sampleRate,
            totalMilliseconds=vowelMilliseconds,
            onsetMilliseconds=int(onset_ms),
            onsetFormants=onsetF,
            onsetBandwidthScale=1.18,
            vowelModel=vowel_kwargs,
        )
        out = _pre_emphasis(out, coefficient=0.86)
        out = _add_breath_noise(out, level_db=-36.0)
        out = _normalize_peak(out, PEAK_DEFAULT)

    elif c == 'y':
        onset_ms = float(consonantMilliseconds) if consonantMilliseconds is not None else 40.0
        onset_ms = max(24.0, min(onset_ms, float(vowelMilliseconds) - 10.0))
        onsetF = GLIDE_ONSETS['y'].get(v, GLIDE_ONSETS['y']['a'])
        out = synth_vowel_with_onset(
            v,
            f0,
            sampleRate,
            totalMilliseconds=vowelMilliseconds,
            onsetMilliseconds=int(onset_ms),
            onsetFormants=onsetF,
            onsetBandwidthScale=1.10,
            vowelModel=vowel_kwargs,
        )
        out = _pre_emphasis(out, coefficient=0.84)
        out = _add_breath_noise(out, level_db=-38.0)
        out = _normalize_peak(out, PEAK_DEFAULT)

    elif c == 'r':
        tap = synth_plosive('t', sampleRate=sampleRate, closureMilliseconds=12.0, burstMilliseconds=6.0,
                            aspirationMilliseconds=4.0, levelDb=-20.0)
        onsetF = LIQUID_ONSETS.get(v, LIQUID_ONSETS['a'])
        vow = synth_vowel_with_onset(
            v,
            f0,
            sampleRate,
            totalMilliseconds=vowelMilliseconds,
            onsetMilliseconds=36,
            onsetFormants=onsetF,
            vowelModel=vowel_kwargs,
        )
        out = _crossfade(tap, vow, sampleRate, overlap_ms=12)

    else:
        raise ValueError("synth_cv: unsupported consonant for synth_cv")

    combined = np.concatenate([head, out]).astype(DTYPE)
    return _normalize_peak(combined, PEAK_DEFAULT)

# =======================
# Higher-level helpers
# =======================

def synth_cv_to_wav(
    cons: str,
    vowel: str,
    outPath: str,
    f0: float = 120.0,
    sampleRate: int = 22050,
    preMilliseconds: int = 0,
    consonantMilliseconds: Optional[int] = None,
    vowelMilliseconds: int = 240,
    overlapMilliseconds: int = 30,
    useOnsetTransition: bool = False,
    vowelModel: Optional[Dict[str, Any]] = None,
    speakerProfile: Optional[SpeakerProfile] = None,
) -> str:
    """CV を合成して WAV 保存"""
    waveform = synth_cv(
        cons,
        vowel,
        f0=f0,
        sampleRate=sampleRate,
        preMilliseconds=preMilliseconds,
        consonantMilliseconds=consonantMilliseconds,
        vowelMilliseconds=vowelMilliseconds,
        overlapMilliseconds=overlapMilliseconds,
        useOnsetTransition=useOnsetTransition,
        vowelModel=vowelModel,
        speakerProfile=speakerProfile,
    )
    return write_wav(outPath, waveform, sampleRate=sampleRate)


def synth_phrase_to_wav(
    vowels: Sequence[str],
    outPath: str,
    f0: float = 120.0,
    unitMilliseconds: int = 220,
    gapMilliseconds: int = 30,
    sampleRate: int = 22050,
    vowelModel: Optional[Dict[str, Any]] = None,
    speakerProfile: Optional[SpeakerProfile] = None,
) -> str:
    """MVP: 母音列 ['a','i',...] からフレーズ WAV を生成"""
    chunks: List[np.ndarray] = []
    vowel_kwargs = dict(vowelModel or {})
    if speakerProfile is not None:
        vowel_kwargs.setdefault('speakerProfile', speakerProfile)

    for v in map(str.lower, vowels):
        if v in VOWEL_TABLE:
            seg = synth_vowel(
                v,
                f0=f0,
                durationSeconds=unitMilliseconds / 1000.0,
                sampleRate=sampleRate,
                **vowel_kwargs,
            )
            chunks.append(seg)
            chunks.append(np.zeros(_ms_to_samples(gapMilliseconds, sampleRate), dtype=DTYPE))
    if not chunks:
        chunks = [np.zeros(int(sampleRate * 0.3), dtype=DTYPE)]
    y = np.concatenate(chunks)
    return write_wav(outPath, y, sampleRate=sampleRate)


def synth_token_sequence(
    tokens: Sequence[str],
    *,
    f0: float = 120.0,
    sampleRate: int = 22050,
    vowelMilliseconds: int = 240,
    overlapMilliseconds: int = 30,
    gapMilliseconds: int = 40,
    useOnsetTransition: bool = False,
    vowelModel: Optional[Dict[str, Any]] = None,
    speakerProfile: Optional[SpeakerProfile] = None,
) -> np.ndarray:
    """ローマ字トークン列から波形を生成するヘルパー。"""
    segs: List[np.ndarray] = []
    gap = _ms_to_samples(max(0, int(gapMilliseconds)), sampleRate)
    vow_sec = max(0.12, float(vowelMilliseconds) / 1000.0)
    nasal_ms = max(80, int(vowelMilliseconds * 0.6))

    vowel_kwargs = dict(vowelModel or {})
    if speakerProfile is not None:
        vowel_kwargs.setdefault('speakerProfile', speakerProfile)
    speaker_profile: Optional[SpeakerProfile] = vowel_kwargs.get('speakerProfile')
    nasal_coupling = speaker_profile.nasal_coupling if speaker_profile is not None else None

    for tok in tokens:
        t = tok.strip().lower()
        if not t:
            continue
        if t == PAUSE_TOKEN:
            pause_ms = max(gapMilliseconds * 3, 120)
            segs.append(np.zeros(_ms_to_samples(pause_ms, sampleRate), dtype=DTYPE))
            continue
        if t in CV_TOKEN_MAP:
            ck, vk = CV_TOKEN_MAP[t]
            seg = synth_cv(
                ck,
                vk,
                f0=f0,
                sampleRate=sampleRate,
                vowelMilliseconds=vowelMilliseconds,
                overlapMilliseconds=overlapMilliseconds,
                useOnsetTransition=useOnsetTransition,
                vowelModel=vowel_kwargs,
                speakerProfile=speaker_profile,
            )
        elif t in VOWEL_TABLE:
            seg = synth_vowel(
                t,
                f0=f0,
                durationSeconds=vow_sec,
                sampleRate=sampleRate,
                **vowel_kwargs,
            )
        elif t in NASAL_TOKEN_MAP:
            seg = synth_nasal(
                NASAL_TOKEN_MAP[t],
                f0=f0,
                durationMilliseconds=nasal_ms,
                sampleRate=sampleRate,
                nasalCoupling=nasal_coupling,
                speakerProfile=speaker_profile,
            )
        else:
            raise ValueError(f"Unsupported token '{tok}' for synthesis")

        if segs and gap > 0:
            segs.append(np.zeros(gap, dtype=DTYPE))
        segs.append(seg.astype(DTYPE, copy=False))

    if not segs:
        return np.zeros(_ms_to_samples(120, sampleRate), dtype=DTYPE)

    y = np.concatenate(segs).astype(DTYPE, copy=False)
    return _normalize_peak(y, PEAK_DEFAULT)


def synth_tokens_to_wav(
    tokens: Sequence[str],
    outPath: str,
    *,
    f0: float = 120.0,
    sampleRate: int = 22050,
    vowelMilliseconds: int = 240,
    overlapMilliseconds: int = 30,
    gapMilliseconds: int = 40,
    useOnsetTransition: bool = False,
    vowelModel: Optional[Dict[str, Any]] = None,
    speakerProfile: Optional[SpeakerProfile] = None,
) -> str:
    """トークン列合成 → WAV 保存。"""
    y = synth_token_sequence(
        tokens,
        f0=f0,
        sampleRate=sampleRate,
        vowelMilliseconds=vowelMilliseconds,
        overlapMilliseconds=overlapMilliseconds,
        gapMilliseconds=gapMilliseconds,
        useOnsetTransition=useOnsetTransition,
        vowelModel=vowelModel,
        speakerProfile=speakerProfile,
    )
    return write_wav(outPath, y, sampleRate=sampleRate)

# =======================
# I/O
# =======================

def write_wav(path: str, audio: np.ndarray, sampleRate: int = 22050) -> str:
    """16-bit PCM で WAV 保存"""
    audio = _ensure_array(audio)
    data16 = np.clip((audio * 32767.0), -32768, 32767).astype(np.int16)
    with wave.open(path, 'wb') as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sampleRate)
        wf.writeframes(data16.tobytes())
    return os.path.abspath(path)